name: School Data Ingestion Pipeline

on:
  workflow_dispatch:
  schedule:
    - cron: "0 6 * * *"   # every day at 06:00 UTC

permissions:
  contents: write
  id-token: write   # required if using OIDC with azure/login

jobs:
  ingest:
    name: Ingest Raw Data & Build Warehouse
    runs-on: ubuntu-latest
    environment: test

    env:
      # Blob (currently from GitHub secrets)
      AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
      AZURE_STORAGE_CONTAINER: ${{ secrets.AZURE_STORAGE_CONTAINER }}

      # Konduit / AKS settings (also in GitHub secrets)
      AKS_RESOURCE_GROUP: ${{ secrets.AKS_RESOURCE_GROUP }}
      AKS_CLUSTER_NAME: ${{ secrets.AKS_CLUSTER_NAME }}
      AKS_NAMESPACE: ${{ secrets.AKS_NAMESPACE }}
      KONDUIT_APP_NAME: ${{ secrets.KONDUIT_APP_NAME }}

    steps:
      # ==============================
      # 1. Checkout repo
      # ==============================
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      # ==============================
      # 2. Install dependencies
      # ==============================
      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: Install .NET SDK
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: "8.0.x"

      - name: Install PowerShell
        run: |
          sudo apt-get update
          sudo apt-get install -y powershell

      - name: Install Azure CLI
        run: |
          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

      # ==============================
      # 3. Azure login (needed for az aks get-credentials)
      # ==============================
      - name: Azure login (OIDC)
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      # ==============================
      # 4. AKS access + konduit install
      # ==============================
      - name: Install kubectl (pinned)
        shell: bash
        run: |
          set -euo pipefail
          KUBECTL_VERSION="v1.29.8"
          curl -fsSLo kubectl "https://dl.k8s.io/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
          sudo install -m 0755 kubectl /usr/local/bin/kubectl
          kubectl version --client=true

      - name: Install kubelogin (pinned)
        shell: bash
        run: |
          set -euo pipefail
          KUBELOGIN_VERSION="v0.1.6"
          curl -fsSLo kubelogin.zip "https://github.com/Azure/kubelogin/releases/download/${KUBELOGIN_VERSION}/kubelogin-linux-amd64.zip"
          unzip -q kubelogin.zip
          sudo install -m 0755 bin/linux_amd64/kubelogin /usr/local/bin/kubelogin
          kubelogin --version

      - name: Configure AKS credentials
        shell: bash
        run: |
          set -euo pipefail
          az aks get-credentials --overwrite-existing -g "${AKS_RESOURCE_GROUP}" -n "${AKS_CLUSTER_NAME}"
          kubelogin convert-kubeconfig -l azurecli

      - name: Download konduit.sh
        shell: bash
        run: |
          set -euo pipefail
          curl -fsSL https://raw.githubusercontent.com/DFE-Digital/teacher-services-cloud/main/scripts/konduit.sh \
            -o "$GITHUB_WORKSPACE/konduit.sh"
          chmod +x "$GITHUB_WORKSPACE/konduit.sh"
          ls -la "$GITHUB_WORKSPACE/konduit.sh"

      # ==============================
      # 5. Ensure Blob container exists (idempotent)
      # ==============================
      - name: Ensure Blob container exists
        shell: pwsh
        run: |
          az storage container create `
            --name $env:AZURE_STORAGE_CONTAINER `
            --connection-string $env:AZURE_STORAGE_CONNECTION_STRING `
            --public-access off | Out-Null

      # ==============================
      # 6. Download raw data files from Blob (ROOT LEVEL)
      # ==============================
      - name: Download raw data files from Blob Storage
        shell: pwsh
        run: |
          $rawDir = "SAPData/DataMap/SourceFiles"
          New-Item -ItemType Directory -Force -Path $rawDir | Out-Null

          Remove-Item "$rawDir/*.csv" -Force -ErrorAction SilentlyContinue

          # List all names, filter in PowerShell
          $all = az storage blob list `
            --container-name $env:AZURE_STORAGE_CONTAINER `
            --connection-string $env:AZURE_STORAGE_CONNECTION_STRING `
            --query "[].name" -o tsv

          $blobs = $all | Where-Object { $_ -notmatch "/" }

          if (-not $blobs) {
            throw "No root-level blobs found in container '$env:AZURE_STORAGE_CONTAINER'."
          }

          foreach ($blob in $blobs) {
            if ($blob -notlike "*.csv") { continue }

            $dest = Join-Path $rawDir $blob

            az storage blob download `
              --container-name $env:AZURE_STORAGE_CONTAINER `
              --name $blob `
              --file $dest `
              --connection-string $env:AZURE_STORAGE_CONNECTION_STRING `
              --overwrite true
          }

          $files = Get-ChildItem $rawDir -Filter *.csv
          if (-not $files) {
            throw "No CSV files were downloaded from Blob to $rawDir."
          }

          Write-Host "Downloaded CSVs:"
          $files | Select-Object Name, Length

      # ==============================
      # 7. Check for data changes (hash-based)
      # ==============================
      - name: Check for data changes
        id: hashcheck
        shell: pwsh
        run: |
          $changed = "UNCHANGED"
          $rawDir = "SAPData/DataMap/SourceFiles"
          $hashDir = "SAPData/Hashes"

          New-Item -ItemType Directory -Force -Path $hashDir | Out-Null

          $files = Get-ChildItem $rawDir -Filter *.csv
          if (-not $files) {
            throw "No CSV files found in $rawDir to hash."
          }

          foreach ($file in $files) {
            $hash = (Get-FileHash $file.FullName -Algorithm SHA256).Hash
            $hashFile = Join-Path $hashDir "$($file.BaseName).hash"

            $existing = $null
            if (Test-Path $hashFile) {
              $existing = (Get-Content $hashFile -Raw).Trim()
            }

            if ($existing -ne $hash) {
              $changed = "CHANGED"
              $hash | Out-File $hashFile -Force -NoNewline
            }
          }

          echo "changed=$changed" >> $env:GITHUB_OUTPUT

      # ==============================
      # 8. Early exit if unchanged
      # ==============================
      - name: Stop workflow if no new data
        if: steps.hashcheck.outputs.changed == 'UNCHANGED'
        run: |
          echo "No raw data changed. Exiting early."
          exit 0

      # ==============================
      # 9. Build + Run SQL generator
      # ==============================
      - name: Build SQL Generator
        if: steps.hashcheck.outputs.changed == 'CHANGED'
        run: |
          dotnet build SAPPub.sln --configuration Release

      - name: Generate SQL Scripts
        if: steps.hashcheck.outputs.changed == 'CHANGED'
        run: |
          dotnet run --configuration Release --project SAPData/SAPData.csproj
          echo "Generated SQL scripts."

      # ==============================
      # 10. Run ETL via konduit (private DB)
      # ==============================
      - name: Run ETL pipeline via konduit
        if: steps.hashcheck.outputs.changed == 'CHANGED'
        shell: bash
        run: |
          set -euo pipefail
          echo "Running ETL via konduit because data changed..."

          RUN_ALL="$(find "$GITHUB_WORKSPACE" -path "*SAPData/bin/*/Sql/run_all.sql" | head -n 1)"

          if [ -z "${RUN_ALL}" ]; then
            echo "ERROR: Could not find generated run_all.sql under SAPData/bin/**/Sql"
            echo "Debug: listing candidate Sql dirs:"
            find "$GITHUB_WORKSPACE/SAPData/bin" -type d -name Sql -maxdepth 6 -print || true
            exit 1
          fi

          echo "Using generated run_all.sql at: ${RUN_ALL}"
          ls -la "$(dirname "$RUN_ALL")" || true

          "$GITHUB_WORKSPACE/konduit.sh" -n "${AKS_NAMESPACE}" -t 7200 -x \
            -i "${RUN_ALL}" \
            "${KONDUIT_APP_NAME}" -- psql


      # ==============================
      # 11. Commit updated hashes
      # ==============================
      - name: Commit updated hashes
        if: steps.hashcheck.outputs.changed == 'CHANGED'
        run: |
          git config --global user.email "actions@github.com"
          git config --global user.name "GitHub Actions"
          git add SAPData/Hashes/*.hash
          git commit -m "Update raw data hashes" || echo "No changes to commit"
          git push || echo "Nothing to push"
