name: School Data Ingestion Pipeline

on:
  workflow_dispatch:
  schedule:
    - cron: "0 6 * * *"   # every day at 06:00 UTC

permissions:
  contents: write
  id-token: write   # required if using OIDC with azure/login

jobs:
  ingest:
    name: Ingest Raw Data & Build Warehouse
    runs-on: ubuntu-latest
    environment: test

    env:
      # Blob (currently from GitHub secrets)
      AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
      AZURE_STORAGE_CONTAINER: ${{ secrets.AZURE_STORAGE_CONTAINER }}

      # Sensitive source URL (ONE endpoint in GitHub secrets)
      SENSITIVE_DATASET_URL: ${{ secrets.SENSITIVE_DATASET_URL }}

      # Konduit / AKS settings (also in GitHub secrets)
      AKS_RESOURCE_GROUP: ${{ secrets.AKS_RESOURCE_GROUP }}
      AKS_CLUSTER_NAME: ${{ secrets.AKS_CLUSTER_NAME }}
      AKS_NAMESPACE: ${{ secrets.AKS_NAMESPACE }}
      KONDUIT_APP_NAME: ${{ secrets.KONDUIT_APP_NAME }}

    steps:
      # ==============================
      # 1. Checkout repo
      # ==============================
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      # ==============================
      # 2. Install dependencies
      # ==============================
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            postgresql-client \
            powershell \
            unzip

      - name: Install .NET SDK
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: "8.0.x"

      - name: Install Azure CLI
        run: |
          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

      # ==============================
      # 3. Azure login (needed for az aks get-credentials)
      # ==============================
      - name: Azure login (OIDC)
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      # ==============================
      # 4. AKS access + konduit install
      # ==============================
      - name: Install kubectl (pinned)
        shell: bash
        run: |
          set -euo pipefail
          KUBECTL_VERSION="v1.29.8"
          curl -fsSLo kubectl "https://dl.k8s.io/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
          sudo install -m 0755 kubectl /usr/local/bin/kubectl
          kubectl version --client=true

      - name: Install kubelogin (pinned)
        shell: bash
        run: |
          set -euo pipefail
          KUBELOGIN_VERSION="v0.1.6"
          curl -fsSLo kubelogin.zip "https://github.com/Azure/kubelogin/releases/download/${KUBELOGIN_VERSION}/kubelogin-linux-amd64.zip"
          unzip -q kubelogin.zip
          sudo install -m 0755 bin/linux_amd64/kubelogin /usr/local/bin/kubelogin
          kubelogin --version

      - name: Configure AKS credentials
        shell: bash
        run: |
          set -euo pipefail
          az aks get-credentials --overwrite-existing -g "${AKS_RESOURCE_GROUP}" -n "${AKS_CLUSTER_NAME}"
          kubelogin convert-kubeconfig -l azurecli

      - name: Download konduit.sh
        shell: bash
        run: |
          set -euo pipefail
          curl -fsSL https://raw.githubusercontent.com/DFE-Digital/teacher-services-cloud/main/scripts/konduit.sh \
            -o "$GITHUB_WORKSPACE/konduit.sh"
          chmod +x "$GITHUB_WORKSPACE/konduit.sh"
          ls -la "$GITHUB_WORKSPACE/konduit.sh"

      # ==============================
      # 5. Ensure Blob container exists (idempotent)
      # ==============================
      - name: Ensure Blob container exists
        shell: pwsh
        run: |
          az storage container create `
            --name $env:AZURE_STORAGE_CONTAINER `
            --connection-string $env:AZURE_STORAGE_CONNECTION_STRING `
            --public-access off | Out-Null


      # ==============================
      # Debug json file parsing - remove later
      # ==============================
      - name: Download raw data files from Blob Storage
        shell: pwsh
        run: |
          $p = "SAPData/raw_sources.json"
          Write-Host "PWD: $(Get-Location)"
          Write-Host "Exists? " (Test-Path $p)
          Write-Host "SHA256:"
          if (Test-Path $p) { (Get-FileHash $p -Algorithm SHA256).Hash } else { "MISSING" }
          Write-Host "First 300 chars:"
          if (Test-Path $p) {
            $c = Get-Content $p -Raw
            $c.Substring(0, [Math]::Min(300, $c.Length)) | Write-Host
          }
          Write-Host "Type:"
          if (Test-Path $p) {
            $o = Get-Content $p -Raw | ConvertFrom-Json
            $o.GetType().FullName | Write-Host
          }


      # ==============================
      # 6. Download source data and store latest versions in Blob
      #
      #    - Reads SAPData/raw_sources.json
      #    - GIAS:
      #        * Resolves dated file using {fileDatePostfix} template
      #        * Walks backwards by day until a valid file is found
      #
      #    - EES:
      #        * Queries API to get latest published dataset version
      #        * Downloads CSV using datasetId + version
      #
      #    - Stores files in Blob using:
      #        <type>_<subtype>_<year>_v<publisherVersion>.csv (EES)
      #        publisher filename with date (GIAS)
      #
      #    - Tracks state in versions.json
      #    - Outputs:
      #         changed=True/False
      #         latest_files={ key : filename }
      # ==============================
      - name: Version datasets from raw_sources.json and upload to Blob
        id: version
        shell: pwsh
        run: |
          $ErrorActionPreference = "Stop"

          $conn = $env:AZURE_STORAGE_CONNECTION_STRING
          $container = $env:AZURE_STORAGE_CONTAINER
          $manifestBlobName = "versions.json"

          $sourcesPathRepo = "SAPData/raw_sources.json"
          if (-not (Test-Path $sourcesPathRepo)) {
            throw "Could not find $sourcesPathRepo in the repo."
          }

          # Separate working dir (do NOT mix with generator input dir)
          $workDir = "SAPData/Work/Versioning"
          New-Item -ItemType Directory -Force -Path $workDir | Out-Null

          # -----------------------------
          # Load sources (robust: array or single object)
          # -----------------------------
          $sourcesRaw = Get-Content $sourcesPathRepo -Raw | ConvertFrom-Json
          if ($null -eq $sourcesRaw) { throw "raw_sources.json parsed to null." }
          if ($sourcesRaw -isnot [System.Array]) { $sourcesRaw = @($sourcesRaw) }

          # Normalize to objects with stable key + metadata
          $sources = @()
          foreach ($s in $sourcesRaw) {
            $type = [string]$s.Type
            $sub  = [string]$s.Subtype
            $year = [string]$s.Year
            $u    = [string]$s.Url

            if ([string]::IsNullOrWhiteSpace($type) -or
                [string]::IsNullOrWhiteSpace($sub)  -or
                [string]::IsNullOrWhiteSpace($year)) {
              throw "Each entry in raw_sources.json must include Type, Subtype, Year. Offending entry: $($s | ConvertTo-Json -Compress)"
            }

            # NOTE: We intentionally do NOT inject SENSITIVE_DATASET_URL here.
            # GIAS is handled later so we can resolve the dated URL safely.

            $composite = "${type}_${sub}_${year}"
            $safeKey = ($composite -replace '[^a-zA-Z0-9_\-\.]', '_').ToLowerInvariant()

            $sources += [pscustomobject]@{
              key       = $safeKey
              url       = $u
              type      = $type
              subtype   = $sub
              year      = $year
              sourceOrg = [string]$s.SourceOrg
              dataSetId = [string]$s.DataSetId
            }
          }

          if ($sources.Count -eq 0) { throw "No sources found in raw_sources.json" }
          Write-Host "Loaded $($sources.Count) sources."

          # -----------------------------
          # Load manifest from Blob (if present)
          # -----------------------------
          $manifestPath = Join-Path $workDir "versions.json"
          $manifest = @{}

          $manifestExists = az storage blob exists `
            --container-name $container `
            --name $manifestBlobName `
            --connection-string $conn | ConvertFrom-Json

          if ($manifestExists.exists -eq $true) {
            az storage blob download `
              --container-name $container `
              --name $manifestBlobName `
              --file $manifestPath `
              --connection-string $conn `
              --overwrite true | Out-Null

            $manifest = Get-Content $manifestPath -Raw | ConvertFrom-Json -AsHashtable
          }

          # -----------------------------
          # Helpers
          # -----------------------------
          function Get-LondonDateYyyyMmDd([int]$daysAgo = 0) {
            $tz = [TimeZoneInfo]::FindSystemTimeZoneById("Europe/London")
            $nowLondon = [TimeZoneInfo]::ConvertTime([DateTimeOffset]::UtcNow, $tz).DateTime.Date
            return $nowLondon.AddDays(-$daysAgo).ToString("yyyyMMdd")
          }

          function Try-Head([string]$testUrl) {
            try {
              $h = Invoke-WebRequest -Method Head -Uri $testUrl -UseBasicParsing
              return @{
                ok   = $true
                etag = $h.Headers.ETag
                len  = $h.Headers.'Content-Length'
                lm   = $h.Headers.'Last-Modified'
              }
            } catch {
              return @{ ok = $false; etag = $null; len = $null; lm = $null }
            }
          }

          function Try-RangeGet([string]$testUrl) {
            try {
              $g = Invoke-WebRequest -Method Get -Uri $testUrl -Headers @{ Range = "bytes=0-0" } -UseBasicParsing
              return @{
                ok   = $true
                etag = $g.Headers.ETag
                len  = $g.Headers.'Content-Length'
                lm   = $g.Headers.'Last-Modified'
              }
            } catch {
              return @{ ok = $false; etag = $null; len = $null; lm = $null }
            }
          }

          function New-EesFileName([string]$type, [string]$subtype, [string]$year, [string]$version) {
            $base = "${type}_${subtype}_${year}"
            $safe = ($base -replace '[^a-zA-Z0-9_\-\.]', '_').ToLowerInvariant()
            return "${safe}_v${version}.csv"
          }

          # -----------------------------
          # Main loop
          # -----------------------------
          $anyChanged = $false
          $latestFiles = @{}

          foreach ($src in $sources) {
            $key  = [string]$src.key
            $type = [string]$src.type
            $sub  = [string]$src.subtype
            $year = [string]$src.year
            $org  = [string]$src.sourceOrg
            $dsid = [string]$src.dataSetId
            $url  = [string]$src.url

            if (-not $manifest.ContainsKey($key)) {
              $manifest[$key] = @{ latest = 0; lastSignature = ""; lastFileName = "" }
            }

            $prevSig = [string]$manifest[$key].lastSignature
            $latest  = [int]$manifest[$key].latest

            $forcedSignature = $null
            $finalName = $null

            # -------------------------------------------------
            # GIAS special handling
            # - url in raw_sources can be "__SECRET_URL__" or SourceOrg="GIAS"
            # - template URL comes from SENSITIVE_DATASET_URL and contains {fileDatePostfix}
            # - preserve original dated filename (no extra version suffix)
            # -------------------------------------------------
            if ($org -eq "GIAS" -or $url -eq "__SECRET_URL__") {
              $isGias = $true

              if ([string]::IsNullOrWhiteSpace($env:SENSITIVE_DATASET_URL)) {
                throw "GIAS requires SENSITIVE_DATASET_URL secret."
              }

              $template = $env:SENSITIVE_DATASET_URL
              if ($template -notmatch "\{fileDatePostfix\}") {
                throw "SENSITIVE_DATASET_URL must include {fileDatePostfix} placeholder."
              }

              # Look back day-by-day starting at today.
              # Cap at last successful date if present, otherwise 30 days.
              $maxLookbackDays = 30
              if ($manifest[$key].ContainsKey("lastSuccessDate")) {
                $lastSuccess = [string]$manifest[$key].lastSuccessDate
                if ($lastSuccess -match '^\d{8}$') {
                  $tz = [TimeZoneInfo]::FindSystemTimeZoneById("Europe/London")
                  $nowLondon = [TimeZoneInfo]::ConvertTime([DateTimeOffset]::UtcNow, $tz).DateTime.Date
                  $lastDt = [DateTime]::ParseExact($lastSuccess, "yyyyMMdd", [System.Globalization.CultureInfo]::InvariantCulture)
                  $d = [int]($nowLondon - $lastDt).TotalDays
                  if ($d -gt 0) { $maxLookbackDays = [Math]::Min([Math]::Max($d, 1), 90) }
                }
              }

              $resolvedDate = $null
              $resolvedUrl = $null
              $resolvedHdr = $null

              for ($i = 0; $i -le $maxLookbackDays; $i++) {
                $d = Get-LondonDateYyyyMmDd -daysAgo $i
                $candidate = $template.Replace("{fileDatePostfix}", $d)

                $hdr = Try-Head $candidate
                if (-not $hdr.ok) { $hdr = Try-RangeGet $candidate }

                if ($hdr.ok) {
                  $resolvedDate = $d
                  $resolvedUrl = $candidate
                  $resolvedHdr = $hdr
                  break
                }
              }

              if (-not $resolvedUrl) {
                throw "GIAS file not found for today or last $maxLookbackDays days (URL hidden)."
              }

              $url = $resolvedUrl
              $manifest[$key].lastSuccessDate = $resolvedDate

              # Preserve original filename exactly (the dated filename in the URL)
              $finalName = [System.IO.Path]::GetFileName($url)

              # Signature includes date + headers
              $parts = @("giasDate:$resolvedDate")
              if ($resolvedHdr.etag) { $parts += "etag:$($resolvedHdr.etag)" }
              if ($resolvedHdr.lm)   { $parts += "lm:$($resolvedHdr.lm)" }
              if ($resolvedHdr.len)  { $parts += "len:$($resolvedHdr.len)" }
              $forcedSignature = ($parts -join "|")
            }

            # -------------------------------------------------
            # EES handling (publisher versions)
            # - use DataSetId -> latest Published version -> download csv
            # - store file as Type_Subtype_Year_v<PublisherVersion>.csv
            # -------------------------------------------------
            if (-not $finalName -and $org -eq "EES" -and -not [string]::IsNullOrWhiteSpace($dsid)) {
              $versionsUrl = "https://api.education.gov.uk/statistics/v1/data-sets/$dsid/versions"
              $versions = Invoke-WebRequest -Uri $versionsUrl -UseBasicParsing |
                Select-Object -ExpandProperty Content | ConvertFrom-Json

              $published = @($versions.results | Where-Object { $_.status -eq "Published" })
              if ($published.Count -eq 0) { throw "No Published versions returned for EES datasetId '$dsid'." }

              $latestPub = $published | Sort-Object { [DateTimeOffset]$_.published } -Descending | Select-Object -First 1
              $dataSetVersion = [string]$latestPub.version

              $url = "https://api.education.gov.uk/statistics/v1/data-sets/$dsid/csv?dataSetVersion=$dataSetVersion"

              $forcedSignature = "ees:$dsid|version:$dataSetVersion"
              $finalName = New-EesFileName -type $type -subtype $sub -year $year -version $dataSetVersion

              $manifest[$key].eesLatestVersion = $dataSetVersion
            }

            # -------------------------------------------------
            # Generic signature (fallback)
            # -------------------------------------------------
            $signature = $null

            if ($forcedSignature) {
              $signature = $forcedSignature
            } else {
              $etag = $null
              $len = $null
              $lm = $null
              $headOk = $false

              try {
                $head = Invoke-WebRequest -Method Head -Uri $url -UseBasicParsing
                $etag = $head.Headers.ETag
                $len  = $head.Headers.'Content-Length'
                $lm   = $head.Headers.'Last-Modified'
                $headOk = $true
              } catch {
                $headOk = $false
              }

              if (-not $headOk -or (-not $etag -and -not $lm -and -not $len)) {
                try {
                  $resp = Invoke-WebRequest -Method Get -Uri $url -Headers @{ Range = "bytes=0-0" } -UseBasicParsing
                  if (-not $etag) { $etag = $resp.Headers.ETag }
                  if (-not $len)  { $len  = $resp.Headers.'Content-Length' }
                  if (-not $lm)   { $lm   = $resp.Headers.'Last-Modified' }
                } catch {
                  # We'll fall back to download if needed
                }
              }

              $sigParts = @()
              if ($etag) { $sigParts += "etag:$etag" }
              if ($lm)   { $sigParts += "lm:$lm" }
              # ignore tiny/likely-wrong lengths (e.g. Range responses returning 1 byte)
              if ($len -and [int64]$len -gt 1024) { $sigParts += "len:$len" }
              $signature = ($sigParts -join "|")
            }

            $needDownloadToDecide = [string]::IsNullOrWhiteSpace($signature)

            # If GIAS, we *do not* use numeric versions for filenames.
            # We still version logically via signature in manifest (lastSignature), and store the dated file name.
            $isChanged = ($latest -eq 0) -or $needDownloadToDecide -or ($signature -ne $prevSig)

            if ($isChanged) {
              # Download full content now
              $tmpPath = Join-Path $workDir "${key}.tmp"
              try {
                Invoke-WebRequest -Uri $url -OutFile $tmpPath -UseBasicParsing
              } catch {
                throw "Failed downloading dataset '$key' from source (URL hidden). $($_.Exception.Message)"
              }

              # Default naming if neither GIAS nor EES set it
              if (-not $finalName) { $finalName = "${key}.csv" }

              $finalPath = Join-Path $workDir $finalName
              Move-Item -Force $tmpPath $finalPath

              az storage blob upload `
                --container-name $container `
                --name $finalName `
                --file $finalPath `
                --connection-string $conn `
                --overwrite | Out-Null

              $anyChanged = $true
              $manifest[$key].latest = ($latest + 1)
              $manifest[$key].lastSignature = $signature
              $manifest[$key].lastFileName = $finalName
              $latestFiles[$key] = $finalName

              Write-Host "UPDATED: $key -> $finalName"
            }
            else {
              # unchanged - keep last known file name
              $lastFile = [string]$manifest[$key].lastFileName
              if ([string]::IsNullOrWhiteSpace($lastFile)) {
                # best-effort fallback
                if ($finalName) { $lastFile = $finalName } else { $lastFile = "${key}.csv" }
              }

              $latestFiles[$key] = $lastFile
              Write-Host "UNCHANGED: $key remains $lastFile"
            }
          }

          # Upload updated manifest if changed
          if ($anyChanged) {
            $manifest | ConvertTo-Json -Depth 30 | Set-Content -Path $manifestPath -Encoding UTF8
            az storage blob upload `
              --container-name $container `
              --name $manifestBlobName `
              --file $manifestPath `
              --connection-string $conn `
              --overwrite | Out-Null
          }

          # Outputs
          "changed=$anyChanged" | Out-File -FilePath $env:GITHUB_OUTPUT -Append -Encoding utf8
          $latestFilesJson = ($latestFiles | ConvertTo-Json -Depth 10 -Compress)
          "latest_files=$latestFilesJson" | Out-File -FilePath $env:GITHUB_OUTPUT -Append -Encoding utf8

      # ==============================
      # 7. Early exit if unchanged
      # ==============================
      - name: Stop workflow if no new data
        if: steps.version.outputs.changed != 'True'
        run: |
          echo "No raw data changed (based on HTTP headers/size). Exiting early."
          exit 0

      # ==============================
      # Remove after testing GIAS 
      # ==============================
      - name: Stop after GIAS test
        if: always()
        run: |
          echo "Stopping after Step 6 for GIAS-only test."
          exit 0

      # ==============================
      # 8. Download the *latest versioned* CSVs from Blob for the generator/ETL
      # ==============================
      - name: Download latest versioned files from Blob Storage
        if: steps.version.outputs.changed == 'True'
        shell: pwsh
        env:
          LATEST_FILES: ${{ steps.version.outputs.latest_files }}
        run: |
          $ErrorActionPreference = "Stop"
          $rawDir = "SAPData/DataMap/SourceFiles"
          New-Item -ItemType Directory -Force -Path $rawDir | Out-Null
          Remove-Item "$rawDir/*" -Force -Recurse -ErrorAction SilentlyContinue

          $latest = $env:LATEST_FILES | ConvertFrom-Json -AsHashtable
          if (-not $latest -or $latest.Keys.Count -eq 0) {
            throw "LATEST_FILES output was empty."
          }

          foreach ($k in $latest.Keys) {
            $blob = [string]$latest[$k]
            if ([string]::IsNullOrWhiteSpace($blob)) {
              throw "LATEST_FILES contained an empty blob name for key '$k'"
            }

            $dest = Join-Path $rawDir $blob

            az storage blob download `
              --container-name $env:AZURE_STORAGE_CONTAINER `
              --name $blob `
              --file $dest `
              --connection-string $env:AZURE_STORAGE_CONNECTION_STRING `
              --overwrite true | Out-Null
          }

          $files = Get-ChildItem $rawDir -File
          if (-not $files) { throw "No files were downloaded from Blob to $rawDir." }

          Write-Host "Downloaded latest files:"
          $files | Select-Object Name, Length


      # ==============================
      # 8b. Normalise filenames for generator (strip _v000001 suffix where present)
      # ==============================
      - name: Normalise filenames for generator (strip _v000001 suffix where present)
        if: steps.version.outputs.changed == 'True'
        shell: pwsh
        run: |
          $ErrorActionPreference = "Stop"
          $rawDir = "SAPData/DataMap/SourceFiles"

          $files = Get-ChildItem $rawDir -Filter *.csv -File
          if (-not $files) {
            throw "No CSV files found in $rawDir"
          }

          foreach ($f in $files) {
            # Only strip numeric version suffix if present: <name>_v000123.csv
            if ($f.BaseName -match '^(?<stable>.+)_v\d{6}$') {
              $stableName = $Matches['stable'] + $f.Extension
              $dest = Join-Path $rawDir $stableName

              # Copy versioned -> stable (overwrite stable)
              Copy-Item -Force $f.FullName $dest
            }
          }

          Write-Host "Normalised files present (post-copy):"
          Get-ChildItem $rawDir -Filter *.csv -File | Select-Object Name, Length

          # Remove ONLY the numeric-versioned copies; keep everything else (e.g. GIAS dated files)
          Get-ChildItem $rawDir -Filter '*_v??????.csv' -File | Remove-Item -Force


      # ==============================
      # 9. Build + Run SQL generator
      # ==============================
      - name: Build SQL Generator
        if: steps.version.outputs.changed == 'True'
        run: |
          dotnet build SAPPub.sln --configuration Release

      - name: Generate SQL Scripts
        if: steps.version.outputs.changed == 'True'
        run: |
          dotnet run --configuration Release --project SAPData/SAPData.csproj
          echo "Generated SQL scripts."

      # ==============================
      # 10. Run ETL via konduit (private DB)
      # ==============================
      - name: Run ETL pipeline via konduit
        if: steps.version.outputs.changed == 'True'
        working-directory: SAPData/Sql
        shell: bash
        run: |
          set -euo pipefail
          echo "Running ETL via konduit because data changed..."
          ls -la "$GITHUB_WORKSPACE/SAPData/Sql" || true
          "$GITHUB_WORKSPACE/konduit.sh" -n "${AKS_NAMESPACE}" -t 7200 -x \
            -i "$GITHUB_WORKSPACE/SAPData/Sql/run_all.sql" \
            "${KONDUIT_APP_NAME}" -- psql
