name: School Data Ingestion Pipeline

on:
  workflow_dispatch:
  schedule:
    - cron: "0 6 * * *"   # every day at 06:00 UTC

permissions:
  contents: write
  id-token: write   # required if using OIDC with azure/login

jobs:
  ingest:
    name: Ingest Raw Data & Build Warehouse
    runs-on: ubuntu-latest
    environment: test

    env:
      # Blob (currently from GitHub secrets)
      AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
      AZURE_STORAGE_CONTAINER: ${{ secrets.AZURE_STORAGE_CONTAINER }}

      # Sensitive source URL (ONE endpoint in GitHub secrets)
      SENSITIVE_DATASET_URL: ${{ secrets.SENSITIVE_DATASET_URL }}

      # Konduit / AKS settings (also in GitHub secrets)
      AKS_RESOURCE_GROUP: ${{ secrets.AKS_RESOURCE_GROUP }}
      AKS_CLUSTER_NAME: ${{ secrets.AKS_CLUSTER_NAME }}
      AKS_NAMESPACE: ${{ secrets.AKS_NAMESPACE }}
      KONDUIT_APP_NAME: ${{ secrets.KONDUIT_APP_NAME }}

    steps:
      # ==============================
      # 1. Checkout repo
      # ==============================
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      # ==============================
      # 2. Install dependencies
      # ==============================
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            postgresql-client \
            powershell \
            unzip

      - name: Install .NET SDK
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: "8.0.x"

      - name: Install Azure CLI
        run: |
          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

      # ==============================
      # 3. Azure login (needed for az aks get-credentials)
      # ==============================
      - name: Azure login (OIDC)
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      # ==============================
      # 4. AKS access + konduit install
      # ==============================
      - name: Install kubectl (pinned)
        shell: bash
        run: |
          set -euo pipefail
          KUBECTL_VERSION="v1.29.8"
          curl -fsSLo kubectl "https://dl.k8s.io/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
          sudo install -m 0755 kubectl /usr/local/bin/kubectl
          kubectl version --client=true

      - name: Install kubelogin (pinned)
        shell: bash
        run: |
          set -euo pipefail
          KUBELOGIN_VERSION="v0.1.6"
          curl -fsSLo kubelogin.zip "https://github.com/Azure/kubelogin/releases/download/${KUBELOGIN_VERSION}/kubelogin-linux-amd64.zip"
          unzip -q kubelogin.zip
          sudo install -m 0755 bin/linux_amd64/kubelogin /usr/local/bin/kubelogin
          kubelogin --version

      - name: Configure AKS credentials
        shell: bash
        run: |
          set -euo pipefail
          az aks get-credentials --overwrite-existing -g "${AKS_RESOURCE_GROUP}" -n "${AKS_CLUSTER_NAME}"
          kubelogin convert-kubeconfig -l azurecli

      - name: Download konduit.sh
        shell: bash
        run: |
          set -euo pipefail
          curl -fsSL https://raw.githubusercontent.com/DFE-Digital/teacher-services-cloud/main/scripts/konduit.sh \
            -o "$GITHUB_WORKSPACE/konduit.sh"
          chmod +x "$GITHUB_WORKSPACE/konduit.sh"
          ls -la "$GITHUB_WORKSPACE/konduit.sh"

      # ==============================
      # 5. Ensure Blob container exists (idempotent)
      # ==============================
      - name: Ensure Blob container exists
        shell: pwsh
        run: |
          az storage container create `
            --name $env:AZURE_STORAGE_CONTAINER `
            --connection-string $env:AZURE_STORAGE_CONNECTION_STRING `
            --public-access off | Out-Null

      # ==============================
      # 6. Download from HTTP sources -> version in Blob -> produce latest set
      #    - uses SAPData/raw_sources.json (27 safe URLs + 1 __SECRET_URL__)
      #    - stores versions.json in Blob
      #    - uploads <type>_<subtype>_<year>_v000001.csv, ...
      #    - outputs:
      #         changed=True/False
      #         latest_files={"composite_key":"composite_key_v000123.csv",...}
      # ==============================
      - name: Version datasets from raw_sources.json and upload to Blob
        id: version
        shell: pwsh
        run: |
          $ErrorActionPreference = "Stop"

          $conn = $env:AZURE_STORAGE_CONNECTION_STRING
          $container = $env:AZURE_STORAGE_CONTAINER
          $manifestBlobName = "versions.json"

          $sourcesPathRepo = "SAPData/raw_sources.json"
          if (-not (Test-Path $sourcesPathRepo)) {
            throw "Could not find $sourcesPathRepo in the repo."
          }

          # Separate working dir (do NOT mix with generator input dir)
          $workDir = "SAPData/Work/Versioning"
          New-Item -ItemType Directory -Force -Path $workDir | Out-Null

          # Load sources
          $sourcesRaw = Get-Content $sourcesPathRepo -Raw | ConvertFrom-Json
          if ($sourcesRaw -isnot [System.Array]) {
            throw "raw_sources.json must be a JSON array."
          }

          # Normalize to { key, url } using composite key: Type_Subtype_Year
          $sources = @()
          foreach ($s in $sourcesRaw) {
            $type = [string]$s.Type
            $sub  = [string]$s.Subtype
            $year = [string]$s.Year
            $u    = [string]$s.Url

            if ([string]::IsNullOrWhiteSpace($type) -or
                [string]::IsNullOrWhiteSpace($sub)  -or
                [string]::IsNullOrWhiteSpace($year) -or
                [string]::IsNullOrWhiteSpace($u)) {
              throw "Each entry in raw_sources.json must include Type, Subtype, Year, Url. Offending entry: $($s | ConvertTo-Json -Compress)"
            }

            $composite = "${type}_${sub}_${year}"
            $safeKey = ($composite -replace '[^a-zA-Z0-9_\-\.]', '_').ToLowerInvariant()

            $sources += [pscustomobject]@{
              key = $safeKey
              url = $u
            }
          }

          if ($sources.Count -eq 0) { throw "No sources found in raw_sources.json" }
          Write-Host "Loaded $($sources.Count) sources."

          # Load manifest from Blob (if present)
          $manifestPath = Join-Path $workDir "versions.json"
          $manifest = @{}

          $manifestExists = az storage blob exists `
            --container-name $container `
            --name $manifestBlobName `
            --connection-string $conn | ConvertFrom-Json

          if ($manifestExists.exists -eq $true) {
            az storage blob download `
              --container-name $container `
              --name $manifestBlobName `
              --file $manifestPath `
              --connection-string $conn `
              --overwrite true | Out-Null

            $manifest = Get-Content $manifestPath -Raw | ConvertFrom-Json -AsHashtable
          }

          $anyChanged = $false
          $latestFiles = @{}

          foreach ($src in $sources) {
            $key = $src.key
            $url = $src.url

            if (-not $manifest.ContainsKey($key)) {
              $manifest[$key] = @{ latest = 0; lastSignature = "" }
            }

            $prevSig = [string]$manifest[$key].lastSignature
            $latest  = [int]$manifest[$key].latest

            # -------------------------------------------------
            # GIAS special handling (template URL + date postfix)
            # -------------------------------------------------
            $forcedSignature = $null
            if ($url -eq "__SECRET_URL__") {
              if ([string]::IsNullOrWhiteSpace($env:SENSITIVE_DATASET_URL)) {
                throw "raw_sources.json contains __SECRET_URL__ but SENSITIVE_DATASET_URL secret is not set."
              }

              $template = $env:SENSITIVE_DATASET_URL
              if ($template -notmatch "\{fileDatePostfix\}") {
                throw "SENSITIVE_DATASET_URL must include {fileDatePostfix} placeholder."
              }

              function Get-LondonDateYyyyMmDd([int]$daysAgo = 0) {
                $tz = [TimeZoneInfo]::FindSystemTimeZoneById("Europe/London")
                $nowLondon = [TimeZoneInfo]::ConvertTime([DateTimeOffset]::UtcNow, $tz).DateTime.Date
                return $nowLondon.AddDays(-$daysAgo).ToString("yyyyMMdd")
              }

              function Try-Get-Headers([string]$testUrl) {
                $etag = $null; $len = $null; $lm = $null
                try {
                  $h = Invoke-WebRequest -Method Head -Uri $testUrl -UseBasicParsing
                  $etag = $h.Headers.ETag
                  $len  = $h.Headers.'Content-Length'
                  $lm   = $h.Headers.'Last-Modified'
                  return @{ ok = $true; etag = $etag; len = $len; lm = $lm }
                } catch {
                  return @{ ok = $false; etag = $null; len = $null; lm = $null }
                }
              }

              function Test-Url-200([string]$testUrl) {
                # Prefer HEAD; fallback to tiny GET
                $hdr = Try-Get-Headers $testUrl
                if ($hdr.ok) { return @{ ok = $true; etag = $hdr.etag; len = $hdr.len; lm = $hdr.lm } }

                try {
                  $g = Invoke-WebRequest -Method Get -Uri $testUrl -Headers @{ Range = "bytes=0-0" } -UseBasicParsing
                  return @{ ok = $true; etag = $g.Headers.ETag; len = $null; lm = $g.Headers.'Last-Modified' }
                } catch {
                  return @{ ok = $false; etag = $null; len = $null; lm = $null }
                }
              }

              # Look back day-by-day starting at today.
              # Cap at last successful date if present, otherwise 30 days.
              $maxLookbackDays = 30
              if ($manifest.ContainsKey($key) -and $manifest[$key].ContainsKey("lastSuccessDate")) {
                $lastSuccess = [string]$manifest[$key].lastSuccessDate
                if ($lastSuccess -match '^\d{8}$') {
                  $tz = [TimeZoneInfo]::FindSystemTimeZoneById("Europe/London")
                  $nowLondon = [TimeZoneInfo]::ConvertTime([DateTimeOffset]::UtcNow, $tz).DateTime.Date
                  $lastDt = [DateTime]::ParseExact($lastSuccess, "yyyyMMdd", [System.Globalization.CultureInfo]::InvariantCulture)
                  $d = [int]($nowLondon - $lastDt).TotalDays
                  if ($d -gt 0) { $maxLookbackDays = [Math]::Min([Math]::Max($d, 1), 90) } # cap sanity
                }
              }

              $resolvedDate = $null
              $resolvedUrl = $null
              $resolvedHdr = $null

              for ($i = 0; $i -le $maxLookbackDays; $i++) {
                $d = Get-LondonDateYyyyMmDd -daysAgo $i
                $candidate = $template.Replace("{fileDatePostfix}", $d)

                $hdr = Test-Url-200 $candidate
                if ($hdr.ok) {
                  $resolvedDate = $d
                  $resolvedUrl = $candidate
                  $resolvedHdr = $hdr
                  break
                }
              }

              if (-not $resolvedUrl) {
                throw "GIAS file not found for today or last $maxLookbackDays days (URL hidden)."
              }

              # Use resolved dated URL for download
              $url = $resolvedUrl

              # Override signature so same-date updates are detected
              $parts = @("giasDate:$resolvedDate")
              if ($resolvedHdr.etag) { $parts += "etag:$($resolvedHdr.etag)" }
              if ($resolvedHdr.lm)   { $parts += "lm:$($resolvedHdr.lm)" }
              if ($resolvedHdr.len)  { $parts += "len:$($resolvedHdr.len)" }
              $forcedSignature = ($parts -join "|")

              # Record last successful date (persisted when manifest uploads)
              $manifest[$key].lastSuccessDate = $resolvedDate
            }

            # -------------------------------------------------
            # Generic signature logic (EES + anything else)
            # -------------------------------------------------
            $etag = $null
            $len = $null
            $lm = $null
            $headOk = $false

            try {
              $head = Invoke-WebRequest -Method Head -Uri $url -UseBasicParsing
              $etag = $head.Headers.ETag
              $len  = $head.Headers.'Content-Length'
              $lm   = $head.Headers.'Last-Modified'
              $headOk = $true
            } catch {
              $headOk = $false
            }

            if (-not $headOk -or (-not $etag -and -not $lm -and -not $len)) {
              try {
                $resp = Invoke-WebRequest -Method Get -Uri $url -Headers @{ Range = "bytes=0-0" } -UseBasicParsing
                if (-not $etag) { $etag = $resp.Headers.ETag }
                if (-not $len)  { $len  = $resp.Headers.'Content-Length' }
                if (-not $lm)   { $lm   = $resp.Headers.'Last-Modified' }
              } catch {
                # We'll fall back to full download if needed
              }
            }

            $sigParts = @()
            if ($etag) { $sigParts += "etag:$etag" }
            if ($lm)   { $sigParts += "lm:$lm" }
            if ($len -and [int64]$len -gt 1024) { $sigParts += "len:$len" }
            $signature = ($sigParts -join "|")

            if ($forcedSignature) { $signature = $forcedSignature }

            $needDownloadToDecide = [string]::IsNullOrWhiteSpace($signature)
            $isChanged = ($latest -eq 0) -or $needDownloadToDecide -or ($signature -ne $prevSig)

            if ($isChanged) {
              $tmpPath = Join-Path $workDir "${key}.tmp.csv"
              try {
                Invoke-WebRequest -Uri $url -OutFile $tmpPath -UseBasicParsing
              } catch {
                # avoid leaking sensitive URL in logs
                throw "Failed downloading dataset '$key' from source (URL hidden). $($_.Exception.Message)"
              }

              $lenActual = (Get-Item $tmpPath).Length
              if (-not $etag -and -not $lm) { $lm = (Get-Date).ToUniversalTime().ToString("r") }

              if (-not $forcedSignature) {
                $sigParts = @()
                if ($etag) { $sigParts += "etag:$etag" }
                if ($lm)   { $sigParts += "lm:$lm" }
                $sigParts += "len:$lenActual"
                $signature = ($sigParts -join "|")
              }

              if ($signature -eq $prevSig -and -not $needDownloadToDecide -and ($latest -gt 0)) {
                Remove-Item -Force $tmpPath
                $v = $latest.ToString("D6")
                $finalName = "${key}_v$v.csv"
                $latestFiles[$key] = $finalName
                Write-Host "UNCHANGED(after download): $key remains $finalName"
                continue
              }

              $anyChanged = $true
              $newVersion = $latest + 1
              $v = $newVersion.ToString("D6")
              $finalName = "${key}_v$v.csv"
              $finalPath = Join-Path $workDir $finalName

              Move-Item -Force $tmpPath $finalPath

              az storage blob upload `
                --container-name $container `
                --name $finalName `
                --file $finalPath `
                --connection-string $conn `
                --overwrite | Out-Null

              $manifest[$key].latest = $newVersion
              $manifest[$key].lastSignature = $signature
              $latestFiles[$key] = $finalName

              Write-Host "UPDATED: $key -> $finalName"
            }
            else {
              $v = $latest.ToString("D6")
              $finalName = "${key}_v$v.csv"
              $latestFiles[$key] = $finalName
              Write-Host "UNCHANGED: $key remains $finalName"
            }
          }

          if ($anyChanged) {
            $manifest | ConvertTo-Json -Depth 10 | Set-Content -Path $manifestPath -Encoding UTF8
            az storage blob upload `
              --container-name $container `
              --name $manifestBlobName `
              --file $manifestPath `
              --connection-string $conn `
              --overwrite | Out-Null
          }

          "changed=$anyChanged" | Out-File -FilePath $env:GITHUB_OUTPUT -Append -Encoding utf8
          $latestFilesJson = ($latestFiles | ConvertTo-Json -Depth 5 -Compress)
          "latest_files=$latestFilesJson" | Out-File -FilePath $env:GITHUB_OUTPUT -Append -Encoding utf8



      # ==============================
      # 7. Early exit if unchanged
      # ==============================
      - name: Stop workflow if no new data
        if: steps.version.outputs.changed != 'True'
        run: |
          echo "No raw data changed (based on HTTP headers/size). Exiting early."
          exit 0

      # ==============================
      # Remove after testing
      # ==============================
      - name: Stop after GIAS test
        if: always()
        run: |
          echo "Stopping after Step 6 for GIAS-only test."
          exit 0

      # ==============================
      # 8. Download the *latest versioned* CSVs from Blob for the generator/ETL
      # ==============================
      - name: Download latest versioned CSVs from Blob Storage
        if: steps.version.outputs.changed == 'True'
        shell: pwsh
        env:
          LATEST_FILES: ${{ steps.version.outputs.latest_files }}
        run: |
          $ErrorActionPreference = "Stop"
          $rawDir = "SAPData/DataMap/SourceFiles"
          New-Item -ItemType Directory -Force -Path $rawDir | Out-Null
          Remove-Item "$rawDir/*.csv" -Force -ErrorAction SilentlyContinue

          $latest = $env:LATEST_FILES | ConvertFrom-Json -AsHashtable
          foreach ($k in $latest.Keys) {
            $blob = $latest[$k]
            $dest = Join-Path $rawDir $blob

            az storage blob download `
              --container-name $env:AZURE_STORAGE_CONTAINER `
              --name $blob `
              --file $dest `
              --connection-string $env:AZURE_STORAGE_CONNECTION_STRING `
              --overwrite true | Out-Null
          }

          $files = Get-ChildItem $rawDir -Filter *.csv
          if (-not $files) { throw "No CSV files were downloaded from Blob to $rawDir." }

          Write-Host "Downloaded latest versioned CSVs:"
          $files | Select-Object Name, Length

      # ==============================
      # 8b. Normalise filenames for generator (strip _v000001 suffix)
      # ==============================
      - name: Normalise CSV filenames (strip version suffix)
        if: steps.version.outputs.changed == 'True'
        shell: pwsh
        run: |
          $ErrorActionPreference = "Stop"
          $rawDir = "SAPData/DataMap/SourceFiles"

          $files = Get-ChildItem $rawDir -Filter *.csv
          if (-not $files) {
            throw "No CSV files found in $rawDir"
          }

          foreach ($f in $files) {
            # Match: <stable>_v000123.csv
            if ($f.BaseName -match '^(?<stable>.+)_v\d{6}$') {
              $stableName = $Matches['stable'] + ".csv"
              $dest = Join-Path $rawDir $stableName

              # Overwrite stable file
              Copy-Item -Force $f.FullName $dest
            } else {
              # If a file doesn't match the pattern, keep as-is
              # (or throw if you want strictness)
              Write-Host "Skipping (no version suffix): $($f.Name)"
            }
          }

          Write-Host "Normalised files present:"
          Get-ChildItem $rawDir -Filter *.csv | Select-Object Name, Length
          Get-ChildItem $rawDir -Filter '*_v??????.csv' | Remove-Item -Force

      # ==============================
      # 9. Build + Run SQL generator
      # ==============================
      - name: Build SQL Generator
        if: steps.version.outputs.changed == 'True'
        run: |
          dotnet build SAPPub.sln --configuration Release

      - name: Generate SQL Scripts
        if: steps.version.outputs.changed == 'True'
        run: |
          dotnet run --configuration Release --project SAPData/SAPData.csproj
          echo "Generated SQL scripts."

      # ==============================
      # 10. Run ETL via konduit (private DB)
      # ==============================
      - name: Run ETL pipeline via konduit
        if: steps.version.outputs.changed == 'True'
        working-directory: SAPData/Sql
        shell: bash
        run: |
          set -euo pipefail
          echo "Running ETL via konduit because data changed..."
          ls -la "$GITHUB_WORKSPACE/SAPData/Sql" || true
          "$GITHUB_WORKSPACE/konduit.sh" -n "${AKS_NAMESPACE}" -t 7200 -x \
            -i "$GITHUB_WORKSPACE/SAPData/Sql/run_all.sql" \
            "${KONDUIT_APP_NAME}" -- psql
